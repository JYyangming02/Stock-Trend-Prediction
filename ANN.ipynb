{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        feature = F.relu(x)\n",
    "        output = self.fc3(feature)\n",
    "        return output, feature\n",
    "\n",
    "def create_lagged_features(df, days=30):\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    for i in tqdm(range(days-1, len(df))):\n",
    "        feature_row = []\n",
    "        target = df.iloc[i]['1_trend']\n",
    "\n",
    "        # 過去days天的數據\n",
    "        for j in range(days-1, -1, -1):\n",
    "            feature_row.extend([\n",
    "                df.iloc[i - j]['close'],\n",
    "                df.iloc[i - j]['open'],\n",
    "                df.iloc[i - j]['high'],\n",
    "                df.iloc[i - j]['low'],\n",
    "                df.iloc[i - j]['volume']\n",
    "            ])\n",
    "        features.append(feature_row)\n",
    "        targets.append(target)\n",
    "\n",
    "    return np.array(features), np.array(targets)\n",
    "\n",
    "def create_test_features(df, days=30):\n",
    "    \n",
    "    features = []\n",
    "    for i in tqdm(range(days-1, len(df)+1, days)):\n",
    "        feature_row = []\n",
    "\n",
    "        for j in range(days-1, -1, -1):\n",
    "            feature_row.extend([\n",
    "                df.iloc[i - j]['close'],\n",
    "                df.iloc[i - j]['open'],\n",
    "                df.iloc[i - j]['high'],\n",
    "                df.iloc[i - j]['low'],\n",
    "                df.iloc[i - j]['volume']\n",
    "            ])\n",
    "\n",
    "        features.append(feature_row)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def train_ann(features_train, targets_train, features_val, targets_val):\n",
    "    input_size = 150\n",
    "    hidden_size = 64\n",
    "    output_size = 3\n",
    "    model = ANN(input_size, hidden_size, output_size)\n",
    "    class_weights = torch.tensor([1.14, 0.79, 1.15], dtype=torch.float32)\n",
    "    criterion = nn.CrossEntropyLoss(class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    epochs = 2000\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(features_train)\n",
    "        loss = criterion(output, targets_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} / {epochs}, loss: {loss.item()}')\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(features_val)\n",
    "            val_loss = criterion(output, targets_val)\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f'Epoch {epoch+1} / {epochs}, val_loss: {val_loss.item()}')\n",
    "\n",
    "    return model\n",
    "def train_rf(features, targets):\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(features, targets)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1833/1833 [00:34<00:00, 53.08it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_df['1_trend'] = train_df['1_trend'].map({-1: 0, 0: 1, 1: 2})\n",
    "train_df['5_trend'] = train_df['5_trend'].map({-1: 0, 0: 1, 1: 2})\n",
    "train_df['10_trend'] = train_df['10_trend'].map({-1: 0, 0: 1, 1: 2})\n",
    "features, targets = create_lagged_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_val, targets_train, targets_val = train_test_split(features, targets, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_train = scaler.fit_transform(features_train)\n",
    "features_val = scaler.transform(features_val)\n",
    "\n",
    "\n",
    "features_train = torch.tensor(features_train, dtype=torch.float32)\n",
    "features_val = torch.tensor(features_val, dtype=torch.float32)\n",
    "\n",
    "targets_train = torch.tensor(targets_train, dtype=torch.long)\n",
    "targets_val = torch.tensor(targets_val, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 / 2000, loss: 0.8207222819328308\n",
      "Epoch 100 / 2000, val_loss: 1.2934614419937134\n",
      "Epoch 200 / 2000, loss: 0.4925943613052368\n",
      "Epoch 200 / 2000, val_loss: 1.7620182037353516\n",
      "Epoch 300 / 2000, loss: 0.30068495869636536\n",
      "Epoch 300 / 2000, val_loss: 2.3843138217926025\n",
      "Epoch 400 / 2000, loss: 0.19178594648838043\n",
      "Epoch 400 / 2000, val_loss: 2.957812786102295\n",
      "Epoch 500 / 2000, loss: 0.11643590033054352\n",
      "Epoch 500 / 2000, val_loss: 3.570354461669922\n",
      "Epoch 600 / 2000, loss: 0.07375786453485489\n",
      "Epoch 600 / 2000, val_loss: 4.170519828796387\n",
      "Epoch 700 / 2000, loss: 0.04777519032359123\n",
      "Epoch 700 / 2000, val_loss: 4.735763072967529\n",
      "Epoch 800 / 2000, loss: 0.031823720782995224\n",
      "Epoch 800 / 2000, val_loss: 5.265988826751709\n",
      "Epoch 900 / 2000, loss: 0.02211838774383068\n",
      "Epoch 900 / 2000, val_loss: 5.713930130004883\n",
      "Epoch 1000 / 2000, loss: 0.01597757637500763\n",
      "Epoch 1000 / 2000, val_loss: 6.112009048461914\n",
      "Epoch 1100 / 2000, loss: 0.01197869423776865\n",
      "Epoch 1100 / 2000, val_loss: 6.470445156097412\n",
      "Epoch 1200 / 2000, loss: 0.009220743551850319\n",
      "Epoch 1200 / 2000, val_loss: 6.79264497756958\n",
      "Epoch 1300 / 2000, loss: 0.0072523560374975204\n",
      "Epoch 1300 / 2000, val_loss: 7.081803798675537\n",
      "Epoch 1400 / 2000, loss: 0.005832860246300697\n",
      "Epoch 1400 / 2000, val_loss: 7.3431901931762695\n",
      "Epoch 1500 / 2000, loss: 0.00477230828255415\n",
      "Epoch 1500 / 2000, val_loss: 7.581961631774902\n",
      "Epoch 1600 / 2000, loss: 0.003966758958995342\n",
      "Epoch 1600 / 2000, val_loss: 7.80360746383667\n",
      "Epoch 1700 / 2000, loss: 0.0033378542866557837\n",
      "Epoch 1700 / 2000, val_loss: 8.01052188873291\n",
      "Epoch 1800 / 2000, loss: 0.002838031155988574\n",
      "Epoch 1800 / 2000, val_loss: 8.205885887145996\n",
      "Epoch 1900 / 2000, loss: 0.002437053946778178\n",
      "Epoch 1900 / 2000, val_loss: 8.387948036193848\n",
      "Epoch 2000 / 2000, loss: 0.0021117995493113995\n",
      "Epoch 2000 / 2000, val_loss: 8.558980941772461\n"
     ]
    }
   ],
   "source": [
    "model = train_ann(features_train, targets_train, features_val, targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN accuracy: 0.3188010899182561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.29      0.28       107\n",
      "           1       0.42      0.39      0.40       154\n",
      "           2       0.24      0.25      0.24       106\n",
      "\n",
      "    accuracy                           0.32       367\n",
      "   macro avg       0.31      0.31      0.31       367\n",
      "weighted avg       0.32      0.32      0.32       367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, feature_extract = model(features_val)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print('ANN accuracy:', accuracy_score(targets_val, predicted))\n",
    "    print(classification_report(targets_val, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 427/427 [00:07<00:00, 57.18it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')\n",
    "test_features = create_test_features(test_df)\n",
    "\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "test_features = torch.tensor(test_features, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, feature_extract = model(test_features)\n",
    "    predicted = torch.argmax(output, 1).numpy()\n",
    "\n",
    "class_map = {0: -1, 1: 0, 2: 1}\n",
    "mapped_predictions = np.array([class_map[p] for p in predicted])\n",
    "\n",
    "output_df = pd.DataFrame({\n",
    "    'id': [i for i in range(len(mapped_predictions))],\n",
    "    'trend': mapped_predictions\n",
    "})\n",
    "\n",
    "output_df.to_csv('./data/ann_predictions1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/ann_predictions1.csv')\n",
    "df2 = pd.read_csv('./data/ann_predictions2.csv')\n",
    "df3 = pd.read_csv('./data/ann_predictions3.csv')\n",
    "\n",
    "\n",
    "combined_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('output.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
